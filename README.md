# Jailbreaking-LLMs

## Data Science for Public Policy Project (I)

Jailbreaks occur when adversarial actors bypass safety restrictions placed on large language models (LLMs), enabling the models to generate harmful content. This emerging phenomenon is at the forefront of trust and safety concerns within the field of generative AI.

### Objectives:

For our project, our group focused on jailbreaks to:

- Discuss the threats posed by LLM misuse and why it's crucial to address the generation of harmful information by generative AI.
- Provide an overview of the safety measures currently in place for LLMs and explore the efforts of tech companies to ensure the safety of their models.
- Discuss and empirically demonstrate the vulnerability of existing approaches to jailbreaks.

### Literature Overview:

**File:** Literature Research - Jailbreaks.pdf

### Dataset:

The complete question set can be found in `forbidden_question_set.csv.zip.`

It consists of 46,800 samples, calculated as follows: 13 scenarios × 30 questions × 5 repetitions × 8 communities × 3 prompts.

The meaning of each column is as follows:

| Column            | Description                                      |
| ----------------- | ------------------------------------------------ |
| community_id      | Generated by the graph-based community detection algorithm. |
| community_name    | Community name referenced in our paper.         |
| prompt_type       | Type of prompt: earliest, latest, or closest one in the community. |
| prompt            | Extracted prompt.                                |
| content_policy_id | Content policy ID.                               |
| content_policy_name | Name of the content policy, e.g., illegal activity. |
| q_id              | Question ID.                                     |
| question          | The question.                                    |
| response_idx      | Index indicating the repetition of each question. |

**File:** forbidden_question_set.csv.zip

### Preprocessing Phase:

We utilized these prompts and the dataset to run experiments using OpenAI API with GPT 3.5 turbo and GPT 4.0 chatbot models. Our aim was to understand which prompts lead to jailbreaks and what distinguishes them from non-jailbreak prompts. We also used the Perspective API to analyze the association between successful jailbreaks and the toxicity level of each prompt.

**Files:**
1) data_analysis.ipynb: Jailbreak attempts via OpenAI API (GPT 3.5 Turbo and GPT 4)
2) toxicity.csv: Toxicity level of all successful jailbreak responses
3) jailbreak_prompts.csv: Prompts resulting in successful jailbreaks
4) regular_prompts.csv: Prompts that do not result in successful jailbreaks
5) classification.ipynb: Comparison of prompts leading to jailbreaks versus those that do not

### Reproducibility and Novelty:

My primary goal was to reproduce results from existing literature while introducing novelty in the data aspect by constructing our dataset. I used Taskade AI, an online prompt generator, to generate prompts across six subjects and tested them for jailbreaks using OpenAI, Meta, and Google LLMs. Additionally, I employed language/sentence classification techniques to identify jailbreak responses. Finally, I filtered the dataset to create visual graphics illustrating the success of jailbreaks across each LLM and highlighting the distinguishing features of prompts more likely to lead to jailbreaks.

**Files:**
1) Prompts.csv (generated data via Taskade AI)
2) Jailbreak.ipynb: Jailbreak attempts via OpenAI API (GPT 3.5 Turbo and GPT 4)
3) Jailbreak 2.ipynb: Jailbreak attempts via Meta and Google API (Llama 2 and Gemini Pro)
4) Descriptive Statistics.ipynb: Analysis of LLMs' vulnerability to jailbreaks and identification of which LLM is more susceptible.

### Group Presentation:

**File:** Presentation - LLM Jailbreaks.pdf

### Collaboration Statement of Group:

**File:** Collaboration Statement.pdf

